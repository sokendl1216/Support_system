"""
AIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆ

æœ¬ç•ªç’°å¢ƒå‘ã‘ã®çµ±åˆãƒ†ã‚¹ãƒˆã¨ã—ã¦ã€ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã®
é€£æºå‹•ä½œã‚’ç·åˆçš„ã«æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import asyncio
import logging
import sys
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger(__name__)

# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ai.cache_manager import AICacheManager
from ai.simplified_high_performance_cache import SimplifiedHighPerformanceCache
from ai.monitoring.cache_dashboard import (
    CacheDashboard, AlertConfig, CacheMetrics, 
    setup_monitoring_dashboard, slack_alert_handler, email_alert_handler
)

class IntegrationTestSuite:
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self):
        self.test_results = []
        self.dashboard = None
        self.cache_managers = []
        self.test_start_time = None
        
    async def setup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger.info("ğŸ”§ ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        test_cache_dir = Path("./test_integration_cache")
        if test_cache_dir.exists():
            import shutil
            shutil.rmtree(test_cache_dir)
        test_cache_dir.mkdir(exist_ok=True)
        
        # è¤‡æ•°ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
        self.cache_managers = [
            AICacheManager(
                cache_dir=str(test_cache_dir / "basic"),
                ttl_seconds=3600,
                max_cache_size_mb=50,
                enable_cache=True
            ),
            SimplifiedHighPerformanceCache(
                cache_dir=str(test_cache_dir / "high_perf"),
                ttl_seconds=3600,
                max_cache_size_mb=50,
                enable_cache=True
            )
        ]
        
        # ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        alert_config = AlertConfig(
            hit_ratio_threshold=0.6,  # ãƒ†ã‚¹ãƒˆç”¨ä½ã„é–¾å€¤
            response_time_threshold_ms=500.0,
            memory_usage_threshold_mb=100.0,
            error_rate_threshold=0.1
        )
        
        self.dashboard = CacheDashboard(
            cache_manager_refs=self.cache_managers,
            alert_config=alert_config,
            metrics_history_size=100
        )
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.dashboard.add_alert_handler(self._test_alert_handler)
        
        # ç›£è¦–é–‹å§‹
        await self.dashboard.start_monitoring()
        self.test_start_time = time.time()
        
        logger.info("âœ… ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
        
    async def _test_alert_handler(self, alert: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆç”¨ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
        logger.warning(f"ğŸš¨ ãƒ†ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆå—ä¿¡: {alert['severity']} - {alert['message']}")
        
    async def test_basic_cache_operations(self) -> bool:
        """åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œãƒ†ã‚¹ãƒˆ"""
        logger.info("\nğŸ“‹ ãƒ†ã‚¹ãƒˆ1: åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ")
        
        try:
            test_data = [
                {"query": f"ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª {i}", "model": "test-model"}
                for i in range(10)
            ]
            
            test_results = [
                {"answer": f"ãƒ†ã‚¹ãƒˆå›ç­” {i}", "tokens": 100 + i * 10}
                for i in range(10)
            ]
            
            # æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆ
            write_times = []
            for i, (key_data, result) in enumerate(zip(test_data, test_results)):
                start_time = time.time()
                
                # å„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«æ›¸ãè¾¼ã¿
                for cache_manager in self.cache_managers:
                    await cache_manager.set_cache(key_data, result)
                
                write_time = time.time() - start_time
                write_times.append(write_time)
                
                if i % 5 == 0:
                    logger.info(f"  æ›¸ãè¾¼ã¿é€²æ—: {i+1}/10 (å¹³å‡æ™‚é–“: {sum(write_times)/len(write_times):.4f}ç§’)")
            
            # èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
            hit_count = 0
            read_times = []
            
            for key_data in test_data:
                start_time = time.time()
                
                # å„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰èª­ã¿è¾¼ã¿
                for cache_manager in self.cache_managers:
                    hit, cached_result = await cache_manager.get_cache(key_data)
                    if hit:
                        hit_count += 1
                
                read_time = time.time() - start_time
                read_times.append(read_time)
            
            # çµæœè©•ä¾¡
            expected_hits = len(test_data) * len(self.cache_managers)
            hit_ratio = hit_count / expected_hits
            avg_write_time = sum(write_times) / len(write_times)
            avg_read_time = sum(read_times) / len(read_times)
            
            logger.info(f"  âœ… ãƒ’ãƒƒãƒˆç‡: {hit_ratio:.2%} ({hit_count}/{expected_hits})")
            logger.info(f"  âœ… å¹³å‡æ›¸ãè¾¼ã¿æ™‚é–“: {avg_write_time:.4f}ç§’")
            logger.info(f"  âœ… å¹³å‡èª­ã¿è¾¼ã¿æ™‚é–“: {avg_read_time:.4f}ç§’")
            
            success = hit_ratio >= 0.95  # 95%ä»¥ä¸Šã®ãƒ’ãƒƒãƒˆç‡ã‚’æœŸå¾…
            self.test_results.append({
                "test_name": "åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ",
                "success": success,
                "hit_ratio": hit_ratio,
                "avg_write_time": avg_write_time,
                "avg_read_time": avg_read_time
            })
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            self.test_results.append({
                "test_name": "åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ",
                "success": False,
                "error": str(e)
            })
            return False
    
    async def test_monitoring_metrics_collection(self) -> bool:
        """ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ†ã‚¹ãƒˆ"""
        logger.info("\nğŸ“Š ãƒ†ã‚¹ãƒˆ2: ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†")
        
        try:
            # åˆæœŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°
            initial_metrics_count = len(self.dashboard.metrics_history)
            
            # 20ç§’é–“ç›£è¦–ã—ã¦ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåé›†ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            logger.info("  20ç§’é–“ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚’è¦³æ¸¬...")
            await asyncio.sleep(22)  # ç›£è¦–é–“éš”10ç§’ + ãƒãƒƒãƒ•ã‚¡
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
            final_metrics_count = len(self.dashboard.metrics_history)
            new_metrics = final_metrics_count - initial_metrics_count
            
            logger.info(f"  âœ… æ–°è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {new_metrics}å€‹")
            
            if new_metrics > 0:
                latest_metrics = self.dashboard.metrics_history[-1]
                logger.info(f"  âœ… æœ€æ–°ãƒ’ãƒƒãƒˆç‡: {latest_metrics.hit_ratio:.2%}")
                logger.info(f"  âœ… æœ€æ–°ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {latest_metrics.response_time_ms:.1f}ms")
                logger.info(f"  âœ… æœ€æ–°ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {latest_metrics.memory_usage_mb:.1f}MB")
                
                success = new_metrics >= 1
            else:
                success = False
                logger.error("  âŒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåé›†ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            self.test_results.append({
                "test_name": "ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†",
                "success": success,
                "new_metrics_count": new_metrics,
                "latest_metrics": latest_metrics.__dict__ if new_metrics > 0 else None
            })
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            self.test_results.append({
                "test_name": "ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†",
                "success": False,
                "error": str(e)
            })
            return False
    
    async def test_alert_system(self) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        logger.info("\nğŸš¨ ãƒ†ã‚¹ãƒˆ3: ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ")
        
        try:
            # åˆæœŸã‚¢ãƒ©ãƒ¼ãƒˆæ•°
            initial_alerts = len(self.dashboard.alerts_history)
            
            # ä½ãƒ’ãƒƒãƒˆç‡ã‚’æ„å›³çš„ã«ä½œæˆï¼ˆã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿã•ã›ã‚‹ï¼‰
            logger.info("  ä½ãƒ’ãƒƒãƒˆç‡çŠ¶æ³ã‚’ä½œæˆä¸­...")
            
            # å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã§ãƒŸã‚¹ã‚’å¤§é‡ç™ºç”Ÿ
            miss_data = [
                {"query": f"å­˜åœ¨ã—ãªã„ã‚¯ã‚¨ãƒª {i}", "model": "test-model"}
                for i in range(20)
            ]
            
            for key_data in miss_data:
                for cache_manager in self.cache_managers:
                    hit, _ = await cache_manager.get_cache(key_data)
                    # æ„å›³çš„ã«ãƒŸã‚¹ç™ºç”Ÿ
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚’å¾…ã¤
            logger.info("  ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿã‚’å¾…æ©Ÿä¸­...")
            await asyncio.sleep(15)  # ç›£è¦–é–“éš”ã‚’å¾…ã¤
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª
            final_alerts = len(self.dashboard.alerts_history)
            new_alerts = final_alerts - initial_alerts
            
            logger.info(f"  âœ… æ–°è¦ã‚¢ãƒ©ãƒ¼ãƒˆ: {new_alerts}å€‹")
            
            if new_alerts > 0:
                for alert in list(self.dashboard.alerts_history)[-new_alerts:]:
                    logger.info(f"    - {alert['severity']}: {alert['type']} - {alert['message']}")
                success = True
            else:
                logger.warning("  âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“ï¼ˆé–¾å€¤èª¿æ•´ãŒå¿…è¦ãªå¯èƒ½æ€§ï¼‰")
                success = False
            
            self.test_results.append({
                "test_name": "ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ",
                "success": success,
                "new_alerts_count": new_alerts,
                "alerts": list(self.dashboard.alerts_history)[-new_alerts:] if new_alerts > 0 else []
            })
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            self.test_results.append({
                "test_name": "ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ",
                "success": False,
                "error": str(e)
            })
            return False
    
    async def test_dashboard_data_generation(self) -> bool:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        logger.info("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆ4: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
        
        try:
            # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—
            status = self.dashboard.get_current_status()
            logger.info(f"  âœ… ç›£è¦–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status['status']}")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦å–å¾—
            summary = self.dashboard.get_performance_summary(hours=1)
            logger.info(f"  âœ… ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {summary.get('data_points', 0)}")
            
            # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            dashboard_file = self.dashboard.dashboard_data_path / "dashboard.json"
            if dashboard_file.exists():
                with open(dashboard_file, 'r', encoding='utf-8') as f:
                    dashboard_data = json.load(f)
                    
                logger.info(f"  âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {dashboard_data.get('last_updated', 'N/A')}")
                
                success = (
                    status['status'] == 'active' and 
                    summary.get('data_points', 0) > 0 and
                    dashboard_data.get('latest_metrics') is not None
                )
            else:
                success = False
                logger.error("  âŒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            self.test_results.append({
                "test_name": "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ",
                "success": success,
                "status": status,
                "summary": summary,
                "dashboard_file_exists": dashboard_file.exists()
            })
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            self.test_results.append({
                "test_name": "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ",
                "success": False,
                "error": str(e)
            })
            return False
    
    async def test_stress_and_performance(self) -> bool:
        """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("\nâš¡ ãƒ†ã‚¹ãƒˆ5: ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        
        try:
            # å¤§é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆ
            request_count = 100
            concurrent_requests = 10
            
            logger.info(f"  {request_count}ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’{concurrent_requests}ä¸¦è¡Œã§å‡¦ç†...")
            
            async def process_request_batch(start_idx: int, batch_size: int):
                """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒãƒƒãƒå‡¦ç†"""
                batch_times = []
                for i in range(start_idx, start_idx + batch_size):
                    key_data = {"query": f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª {i}", "model": "stress-test"}
                    result_data = {"answer": f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå›ç­” {i}", "tokens": 150}
                    
                    start_time = time.time()
                    
                    # æ›¸ãè¾¼ã¿
                    for cache_manager in self.cache_managers:
                        await cache_manager.set_cache(key_data, result_data)
                    
                    # èª­ã¿è¾¼ã¿
                    for cache_manager in self.cache_managers:
                        await cache_manager.get_cache(key_data)
                    
                    batch_times.append(time.time() - start_time)
                
                return batch_times
            
            # ä¸¦è¡Œå‡¦ç†å®Ÿè¡Œ
            batch_size = request_count // concurrent_requests
            tasks = [
                process_request_batch(i * batch_size, batch_size)
                for i in range(concurrent_requests)
            ]
            
            start_time = time.time()
            batch_results = await asyncio.gather(*tasks)
            total_time = time.time() - start_time
            
            # çµæœé›†è¨ˆ
            all_times = []
            for batch_times in batch_results:
                all_times.extend(batch_times)
            
            avg_request_time = sum(all_times) / len(all_times)
            max_request_time = max(all_times)
            throughput = request_count / total_time
            
            logger.info(f"  âœ… ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
            logger.info(f"  âœ… å¹³å‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚é–“: {avg_request_time:.4f}ç§’")
            logger.info(f"  âœ… æœ€å¤§ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚é–“: {max_request_time:.4f}ç§’")
            logger.info(f"  âœ… ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}req/sec")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–
            success = (
                avg_request_time < 0.1 and  # å¹³å‡100msæœªæº€
                max_request_time < 0.5 and  # æœ€å¤§500msæœªæº€
                throughput > 10              # 10req/secä»¥ä¸Š
            )
            
            self.test_results.append({
                "test_name": "ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                "success": success,
                "total_time": total_time,
                "avg_request_time": avg_request_time,
                "max_request_time": max_request_time,
                "throughput": throughput,
                "request_count": request_count
            })
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            self.test_results.append({
                "test_name": "ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                "success": False,
                "error": str(e)
            })
            return False
    
    async def generate_test_report(self):
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("\nğŸ“„ ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['success'])
        test_duration = time.time() - self.test_start_time if self.test_start_time else 0
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
                "test_duration_seconds": test_duration,
                "timestamp": datetime.now().isoformat()
            },
            "test_results": self.test_results,
            "monitoring_status": self.dashboard.get_current_status() if self.dashboard else None,
            "performance_summary": self.dashboard.get_performance_summary(hours=1) if self.dashboard else None
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = Path("./test_integration_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º
        logger.info("=" * 60)
        logger.info("ğŸ¯ çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 60)
        logger.info(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        logger.info(f"æˆåŠŸ: {passed_tests} âœ…")
        logger.info(f"å¤±æ•—: {total_tests - passed_tests} âŒ")
        logger.info(f"æˆåŠŸç‡: {passed_tests/total_tests:.1%}")
        logger.info(f"å®Ÿè¡Œæ™‚é–“: {test_duration:.1f}ç§’")
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")
        
        # å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ
        for result in self.test_results:
            status = "âœ…" if result['success'] else "âŒ"
            logger.info(f"  {status} {result['test_name']}")
            
        return report
    
    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("\nğŸ§¹ ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
        
        if self.dashboard:
            await self.dashboard.stop_monitoring()
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        test_cache_dir = Path("./test_integration_cache")
        if test_cache_dir.exists():
            import shutil
            shutil.rmtree(test_cache_dir)
        
        logger.info("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

async def run_integration_tests():
    """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ AIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    test_suite = IntegrationTestSuite()
    
    try:
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        await test_suite.setup_test_environment()
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_functions = [
            test_suite.test_basic_cache_operations,
            test_suite.test_monitoring_metrics_collection,
            test_suite.test_alert_system,
            test_suite.test_dashboard_data_generation,
            test_suite.test_stress_and_performance
        ]
        
        for test_func in test_functions:
            try:
                await test_func()
            except Exception as e:
                logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {test_func.__name__}: {e}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = await test_suite.generate_test_report()
        
        return report
        
    finally:
        await test_suite.cleanup()

if __name__ == "__main__":
    asyncio.run(run_integration_tests())
